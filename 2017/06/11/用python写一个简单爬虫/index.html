<!DOCTYPE html><html><head><meta charset="UTF-8"><meta content="width=device-width, initial-scale=1.0,  user-scalable=0" name="viewport"><title>KIT</title><link rel="stylesheet" href="/css/index.min.css"></head><body><nav class="nav-button-list"><div class="btn menu show-canvas"><span class="icon-bar"></span></div><div class="btn top btn-go-top"><i class="ion-ios-arrow-up"></i></div></nav><div class="off-canvas-wrap"><div class="off-canvas-content"><main class="post-container"><div class="post"><header class="post-header"><h1 class="post-title">用python写一个简单爬虫</h1><section class="post-meta"><i class="ion-ios-clock-outline"></i><time class="post-date">Jun 11, 2017</time><span class="dot-sep">·</span><i class="ion-ios-pricetags-outline"></i><a href="/tags/python/">python</a></section></header><article data-spy="scroll" data-target="panel1" class="content"><p>之前一直有学习python，但是一直都是在学一些比较基础的东西，所在在这段空闲的时间打算系统学一下python。在这会使用python写一个爬百科的简单爬虫，首先分析下爬虫的需求。</p>
<h1 id="主要需求"><a href="#主要需求" class="headerlink" title="主要需求"></a>主要需求</h1><p>这个demo中爬虫有3个主要模块，<strong>URL管理器</strong>、<strong>下载器</strong>、<strong>解析器</strong></p>
<ul>
<li>URL管理器主要处理爬取的URL的状态。</li>
<li>下载器会通过<strong>URL管理器</strong>传送过来有效URL进行下载。</li>
<li>解析器会将<strong>下载器</strong>下载的内容解析成字符串，再进行保存。如果解析出有效URL再传给<strong>URL管理器</strong>进行重复操作。</li>
</ul>
<a id="more"></a>
<h2 id="入口程序"><a href="#入口程序" class="headerlink" title="入口程序"></a>入口程序</h2><p><code>spider_main</code>文件为程序入口，处理整个爬虫的运行逻辑</p>
<p></p><p class="code-caption" data-lang="python" data-line_number="backend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span></p><p></p>
<pre><code class="python">import url_manager, html_downloader, html_outputer, html_parser
import sys

class SpiderMain(object):
    def __init__(self):
        self.urls = url_manager.UrlManager()
        self.downloder = html_downloader.HtmlDownloader()
        self.parser = html_parser.HtmlParser()
        self.outputer = html_outputer.HtmlOutputer()

    def crawl(self, root_url):
        count = 1
        # 初始化原始的爬取地址
        self.urls.add_new_ursl(root_url)

        while self.urls.has_new_url():
            try:
                new_url = self.urls.get_new_url()
                print &#39;crawl %d : %s&#39; % (count, new_url)
                html_cont = self.downloder.download(new_url)
                new_urls, new_data = self.parser.parse(new_url, html_cont)

                self.urls.add_new_urls(new_urls)
                self.outputer.collect_data(new_data)

                # 爬取的数据到1000条时跳出循环
                if count == 1000:
                    break

                count = count + 1
            except:
                # 使用sys抛出其他函数错误
                info=sys.exc_info()  
                print info[0],&quot;:&quot;,info[1]

                print &#39;crawl failed&#39;

        # 将爬取的数据组装
        self.outputer.output_html()

if __name__ == &#39;__main__&#39;:
    root_url = &#39;http://baike.baidu.com/item/Python&#39;
    obj_spider = SpiderMain()
    obj_spider.crawl(root_url)
</code></pre>
<h2 id="url管理器"><a href="#url管理器" class="headerlink" title="url管理器"></a>url管理器</h2><p></p><p class="code-caption" data-lang="python" data-line_number="backend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span></p><p></p>
<pre><code class="python"># url_manager.py
class UrlManager(object):
    def __init__(self):
        self.new_urls = set()
        self.old_urls = set()

    # 添加一条新的url
    def add_new_url(self, url):
        if url is None:
            return

        if url not in self.new_urls and url not in self.old_urls:
            self.new_urls.add(url)

    # 添加多条url
    def add_new_urls(self, urls):
        if urls is None and len(urls) == 0:
            return

        for url in urls:
            self.add_new_url(url)  

    # 是否有代爬取地址
    def has_new_url(self):
        return len(self.new_urls) != 0

    # 返回一条新的url
    def get_new_url(self):
        new_url = self.new_urls.pop()
        self.old_urls.add(new_url)
        return new_url
</code></pre>
<h2 id="下载器"><a href="#下载器" class="headerlink" title="下载器"></a>下载器</h2><p>下载器使用自带的<a href="https://docs.python.org/2/library/urllib2.html" target="_blank" rel="external">urllib2</a>库处理请求</p>
<p></p><p class="code-caption" data-lang="python" data-line_number="backend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span></p><p></p>
<pre><code class="python"># html_downloader.py
import urllib2

class HtmlDownloader(object):
    def download(self, url):
        if url is None:
            return

        # 这里设置一下超时，不然网络会使程序运行出现停止
        response = urllib2.urlopen(url, timeout = 5)

        # 请求失败时候跳出程序
        if response.getcode() != 200:
            return None

        return response.read()
</code></pre>
<h2 id="解析器"><a href="#解析器" class="headerlink" title="解析器"></a>解析器</h2><p>解析器分为两部分，一个解析，一个输出，首先看看解析模块。</p>
<p>解析html我们使用了<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html" target="_blank" rel="external">beautifulsoup</a>，解析后可以使用库提供的方法做DOM操作，获取有用的信息</p>
<p></p><p class="code-caption" data-lang="python" data-line_number="backend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span></p><p></p>
<pre><code class="python"># html_parser.py
from bs4 import BeautifulSoup
import re
import urlparse

class HtmlParser(object):
    def _get_new_urls(self, page_url, soup):
        new_urls = set()

        # 获取a标签中所有匹配的url地址
        links = soup.find_all(&#39;a&#39;, href=re.compile(r&#39;/item/&#39;))
        for link in links:
             new_url = link[&#39;href&#39;]
             new_full_url = urlparse.urljoin(page_url, new_url)
             new_urls.add(new_full_url)

        return new_urls

    def _get_new_data(self, page_url, soup):
        res_data = {}

        # 获取页面相关内容
        res_data[&#39;url&#39;] = page_url

        # &lt;dd class=&quot;lemmaWgt-lemmaTitle-title&quot;&gt; &lt;h1&gt;Python&lt;/h1&gt;
        title_node = soup.find(&#39;dd&#39;, class_=&#39;lemmaWgt-lemmaTitle-title&#39;).find(&#39;h1&#39;)
        res_data[&#39;title&#39;] = title_node.get_text()

        # &lt;div class=&quot;lemma-summary&quot; label-module=&quot;lemmaSummary&quot;&gt;
        summary_node = soup.find(&#39;div&#39;, class_=&#39;lemma-summary&#39;)
        res_data[&#39;summary&#39;] = summary_node.get_text()

        return res_data

    def parse(self, page_url, html_cont):
        if page_url is None or html_cont is None:
            return

        soup = BeautifulSoup(html_cont, &#39;html.parser&#39;, from_encoding=&#39;utf-8&#39;)
        new_urls = self._get_new_urls(page_url, soup)
        new_data = self._get_new_data(page_url, soup)

        return new_urls, new_data
</code></pre>
<pre><code class="python"># html_outputer.py
class HtmlOutputer(object):
    def __init__(self):
        self.datas = []

    # 获取到的页面内容对象push到数组中
    def collect_data(self, data):
        if data is None:
            return

        self.datas.append(data)

    # 将数组循环拼接，写入html文件中
    def output_html(self):
        fout = open(&#39;output.html&#39;, &#39;w&#39;)

        fout.write(&#39;&lt;html&gt;&#39;)
        fout.write(&#39;&lt;body&gt;&#39;)
        fout.write(&#39;&lt;table&gt;&#39;)

        for data in self.datas:
            fout.write(&#39;&lt;tr&gt;&#39;)
            fout.write(&#39;&lt;td&gt;%s&lt;/td&gt;&#39;%data[&#39;url&#39;])
            fout.write(&#39;&lt;td&gt;%s&lt;/td&gt;&#39;%data[&#39;title&#39;].encode(&#39;utf-8&#39;))
            fout.write(&#39;&lt;td&gt;%s&lt;/td&gt;&#39;%data[&#39;summary&#39;].encode(&#39;utf-8&#39;))
            fout.write(&#39;&lt;/tr&gt;&#39;) 

        fout.write(&#39;&lt;/html&gt;&#39;)
        fout.write(&#39;&lt;/body&gt;&#39;)
        fout.write(&#39;&lt;/table&gt;&#39;)
</code></pre>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>短短几行代码就实现了一个简单的爬虫，足以体现python的简单易用。学习python主要是想了解一下机器学习，后面会看一些书籍和资料，应该会再记录，但是后面估计会先写完深度学习javascript系列先。</p>
</article></div></main><div class="page-footer"><div class="post-pagination"><a href="/2017/06/14/深入学习javascript-闭包/"><i class="ion-arrow-left-c"></i>深入学习javascript-闭包</a><a href="/2017/05/31/深入学习javascript-作用域/">深入学习javascript--作用域<i class="ion-arrow-right-c"></i></a></div></div><div class="cp"><div>©   2015 - 2016   Kit</div><div><span>power by </span><span><a href="//hexo.io">hexo.</a></span><span>Design By Kit.</span></div></div></div><div class="off-canvas"><div id="post-side-bar" class="post-side-bar"><div class="tab-btn"><button class="tab-item active">文章目录</button><button class="tab-item">站点导航</button></div><div class="tab-content"><div id="panel1" class="side-bar-panel active"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#主要需求"><span class="nav-number">1.</span> <span class="nav-text">主要需求</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#入口程序"><span class="nav-number">1.1.</span> <span class="nav-text">入口程序</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#url管理器"><span class="nav-number">1.2.</span> <span class="nav-text">url管理器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#下载器"><span class="nav-number">1.3.</span> <span class="nav-text">下载器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#解析器"><span class="nav-number">1.4.</span> <span class="nav-text">解析器</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#总结"><span class="nav-number">2.</span> <span class="nav-text">总结</span></a></li></ol></div><div id="panel2" class="side-bar-panel"><ul class="post-side-list"><li><div class="head"><img src="/img/head.jpg" class="img-responsive"></div></li><li class="overview"><a><div>9</div><div>日志</div></a><a class="m unopen"><div>4</div><div>分类</div></a><a><div>7</div><div>标签</div></a></li><li><a href="/" class="item"><i class="ion-help-buoy"></i><span>Home</span></a><a href="/archives" class="item m"><i class="ion-nuclear"></i><span>Archives</span></a><a href="//github.com/kitwon" class="item"><i class="ion-social-github"></i><span>Github</span></a></li></ul></div></div></div></div></div><script src="/js/vendor.min.js"></script><script src="/js/post.min.js"></script></body></html>