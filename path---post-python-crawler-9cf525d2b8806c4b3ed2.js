webpackJsonp([0x72959fd11add],{301:function(n,e){n.exports={data:{markdownRemark:{html:"<p>之前一直有学习python，但是一直都是在学一些比较基础的东西，所在在这段空闲的时间打算系统学一下python。在这会使用python写一个爬百科的简单爬虫，首先分析下爬虫的需求。</p>\n<h1>主要需求</h1>\n<p>这个demo中爬虫有3个主要模块，<strong>URL管理器</strong>、<strong>下载器</strong>、<strong>解析器</strong></p>\n<ul>\n<li>URL管理器主要处理爬取的URL的状态。</li>\n<li>下载器会通过<strong>URL管理器</strong>传送过来有效URL进行下载。</li>\n<li>解析器会将<strong>下载器</strong>下载的内容解析成字符串，再进行保存。如果解析出有效URL再传给<strong>URL管理器</strong>进行重复操作。</li>\n</ul>\n<h2>入口程序</h2>\n<p><code>spider_main</code>文件为程序入口，处理整个爬虫的运行逻辑</p>\n<!-- more -->\n<pre><code class=\"language-python\">import url_manager, html_downloader, html_outputer, html_parser\nimport sys\n\nclass SpiderMain(object):\n    def __init__(self):\n        self.urls = url_manager.UrlManager()\n        self.downloder = html_downloader.HtmlDownloader()\n        self.parser = html_parser.HtmlParser()\n        self.outputer = html_outputer.HtmlOutputer()\n\n    def crawl(self, root_url):\n        count = 1\n        # 初始化原始的爬取地址\n        self.urls.add_new_ursl(root_url)\n\n        while self.urls.has_new_url():\n            try:\n                new_url = self.urls.get_new_url()\n                print 'crawl %d : %s' % (count, new_url)\n                html_cont = self.downloder.download(new_url)\n                new_urls, new_data = self.parser.parse(new_url, html_cont)\n\n                self.urls.add_new_urls(new_urls)\n                self.outputer.collect_data(new_data)\n\n                # 爬取的数据到1000条时跳出循环\n                if count == 1000:\n                    break\n\n                count = count + 1\n            except:\n                # 使用sys抛出其他函数错误\n                info=sys.exc_info()\n                print info[0],\":\",info[1]\n\n                print 'crawl failed'\n\n        # 将爬取的数据组装\n        self.outputer.output_html()\n\nif __name__ == '__main__':\n    root_url = 'http://baike.baidu.com/item/Python'\n    obj_spider = SpiderMain()\n    obj_spider.crawl(root_url)\n</code></pre>\n<h2>url管理器</h2>\n<pre><code class=\"language-python\"># url_manager.py\nclass UrlManager(object):\n    def __init__(self):\n        self.new_urls = set()\n        self.old_urls = set()\n\n    # 添加一条新的url\n    def add_new_url(self, url):\n        if url is None:\n            return\n\n        if url not in self.new_urls and url not in self.old_urls:\n            self.new_urls.add(url)\n\n    # 添加多条url\n    def add_new_urls(self, urls):\n        if urls is None and len(urls) == 0:\n            return\n\n        for url in urls:\n            self.add_new_url(url)\n\n    # 是否有代爬取地址\n    def has_new_url(self):\n        return len(self.new_urls) != 0\n\n    # 返回一条新的url\n    def get_new_url(self):\n        new_url = self.new_urls.pop()\n        self.old_urls.add(new_url)\n        return new_url\n</code></pre>\n<h2>下载器</h2>\n<p>下载器使用自带的<a href=\"https://docs.python.org/2/library/urllib2.html\">urllib2</a>库处理请求</p>\n<pre><code class=\"language-python\"># html_downloader.py\nimport urllib2\n\nclass HtmlDownloader(object):\n    def download(self, url):\n        if url is None:\n            return\n\n        # 这里设置一下超时，不然网络会使程序运行出现停止\n        response = urllib2.urlopen(url, timeout = 5)\n\n        # 请求失败时候跳出程序\n        if response.getcode() != 200:\n            return None\n\n        return response.read()\n</code></pre>\n<h2>解析器</h2>\n<p>解析器分为两部分，一个解析，一个输出，首先看看解析模块。</p>\n<p>解析html我们使用了<a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html\">beautifulsoup</a>，解析后可以使用库提供的方法做DOM操作，获取有用的信息</p>\n<pre><code class=\"language-python\"># html_parser.py\nfrom bs4 import BeautifulSoup\nimport re\nimport urlparse\n\nclass HtmlParser(object):\n    def _get_new_urls(self, page_url, soup):\n        new_urls = set()\n\n        # 获取a标签中所有匹配的url地址\n        links = soup.find_all('a', href=re.compile(r'/item/'))\n        for link in links:\n            new_url = link['href']\n            new_full_url = urlparse.urljoin(page_url, new_url)\n            new_urls.add(new_full_url)\n\n        return new_urls\n\n    def _get_new_data(self, page_url, soup):\n        res_data = {}\n\n        # 获取页面相关内容\n        res_data['url'] = page_url\n\n        # &#x3C;dd class=\"lemmaWgt-lemmaTitle-title\"> &#x3C;h1>Python&#x3C;/h1>\n        title_node = soup.find('dd', class_='lemmaWgt-lemmaTitle-title').find('h1')\n        res_data['title'] = title_node.get_text()\n\n        # &#x3C;div class=\"lemma-summary\" label-module=\"lemmaSummary\">\n        summary_node = soup.find('div', class_='lemma-summary')\n        res_data['summary'] = summary_node.get_text()\n\n        return res_data\n\n    def parse(self, page_url, html_cont):\n        if page_url is None or html_cont is None:\n            return\n\n        soup = BeautifulSoup(html_cont, 'html.parser', from_encoding='utf-8')\n        new_urls = self._get_new_urls(page_url, soup)\n        new_data = self._get_new_data(page_url, soup)\n\n        return new_urls, new_data\n</code></pre>\n<pre><code class=\"language-python\"># html_outputer.py\nclass HtmlOutputer(object):\n    def __init__(self):\n        self.datas = []\n\n    # 获取到的页面内容对象push到数组中\n    def collect_data(self, data):\n        if data is None:\n            return\n\n        self.datas.append(data)\n\n    # 将数组循环拼接，写入html文件中\n    def output_html(self):\n        fout = open('output.html', 'w')\n\n        fout.write('&#x3C;html>')\n        fout.write('&#x3C;body>')\n        fout.write('&#x3C;table>')\n\n        for data in self.datas:\n            fout.write('&#x3C;tr>')\n            fout.write('&#x3C;td>%s&#x3C;/td>'%data['url'])\n            fout.write('&#x3C;td>%s&#x3C;/td>'%data['title'].encode('utf-8'))\n            fout.write('&#x3C;td>%s&#x3C;/td>'%data['summary'].encode('utf-8'))\n            fout.write('&#x3C;/tr>')\n\n        fout.write('&#x3C;/html>')\n        fout.write('&#x3C;/body>')\n        fout.write('&#x3C;/table>')\n</code></pre>\n<h1>总结</h1>\n<p>短短几行代码就实现了一个简单的爬虫，足以体现python的简单易用。学习python主要是想了解一下机器学习，后面会看一些书籍和资料，应该会再记录，但是后面估计会先写完深度学习javascript系列先。</p>",frontmatter:{date:"June 11, 2017",path:"/post/python-crawler",title:"用python写一个简单爬虫"},headings:[{depth:1,value:"主要需求"},{depth:2,value:"入口程序"},{depth:2,value:"url管理器"},{depth:2,value:"下载器"},{depth:2,value:"解析器"},{depth:1,value:"总结"}]}},pathContext:{prev:{title:"深入学习javascript-闭包",date:"2017-06-14",category:null,tags:null,path:"/post/js-closures"},next:{title:"深入学习javascript--作用域",date:"2017-05-31",category:null,tags:null,path:"/post/js-scope"}}}}});
//# sourceMappingURL=path---post-python-crawler-9cf525d2b8806c4b3ed2.js.map